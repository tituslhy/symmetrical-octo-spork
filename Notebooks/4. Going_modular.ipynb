{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim is to turn useful notebook code cells into reusable python files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tituslim/Documents/Personal Learning Folder/Data Science/15. PyTorch Developer Class/symmetrical-octo-spork/Notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datasets and dataloaders using `%%writefile` magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists\n"
     ]
    }
   ],
   "source": [
    "# Create directory for going_modular\n",
    "import os\n",
    "if not os.path.exists(\"../going_modular\"):\n",
    "    os.makedirs(\"../going_modular\")\n",
    "else:\n",
    "    print(\"Directory exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../going_modular/get_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../going_modular/get_data.py\n",
    "\"\"\"Extracts pizza steak sushi data file\n",
    "\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "def main(url: str,\n",
    "         data_path: str = \"data\",\n",
    "         image_dir: str = \"pizza_steak_sushi\"):\n",
    "    \n",
    "    data_path = Path(\"data/\")\n",
    "    image_path = data_path/image_dir\n",
    "\n",
    "    if image_path.is_dir():\n",
    "        print(f\"{image_path} directory already exists. Skipping download...\")\n",
    "    else:\n",
    "        print(f\"Creating {image_path} directory...\")\n",
    "        image_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Download data\n",
    "    with open(data_path/\"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "        request = requests.get(url)\n",
    "        print(\"Downloading pizza_steak_sushi data...\")\n",
    "        f.write(request.content)\n",
    "\n",
    "    # Unzip zip folder\n",
    "    with zipfile.ZipFile(data_path/\"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
    "        print(\"Unziping pizza_steak_sushi data...\")\n",
    "        zip_ref.extractall(image_path)\n",
    "    \n",
    "    return {\"status\": \"Data extracted successfully\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../going_modular/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../going_modular/data_setup.py\n",
    "\"\"\"\n",
    "Contains functionality for creating PyTorch DataLoaders for\n",
    "image classification tasks.\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "from typing import Tuple, List\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str,\n",
    "    test_dir: str,\n",
    "    transform: transforms.Compose,\n",
    "    batch_size: int,\n",
    "    num_workers: int\n",
    ") -> Tuple[torch.utils.data.DataLoader, \n",
    "           torch.utils.data.DataLoader,\n",
    "           List]:\n",
    "    \"\"\"Creates torch datasets and subsequently dataloaders for training\n",
    "    and testing sets. \n",
    "\n",
    "    Args:\n",
    "        train_dir (str): Filepath to train data\n",
    "        test_dir (str): Filepath to test data\n",
    "        transform (transforms.Compose): torch transforms.Compose object\n",
    "        batch_size: Number of samples per batch in each of the datalaoders\n",
    "        num_workers (int, optional): Defaults to NUM_WORKERS.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_dataloader, test_dataloader, class_names) where\n",
    "        classnames is a list of the target classes.\n",
    "    \"\"\"\n",
    "    train_data = datasets.ImageFolder(root = train_dir,\n",
    "                                      transform = transform)\n",
    "    class_names = train_data.classes\n",
    "    test_data = datasets.ImageFolder(root = test_dir,\n",
    "                                      transform = transform)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_data,\n",
    "                                  batch_size = batch_size,\n",
    "                                  shuffle = True,\n",
    "                                  num_workers = num_workers,\n",
    "                                  pin_memory = True\n",
    "                                  )\n",
    "    \n",
    "    test_dataloader = DataLoader(test_data,\n",
    "                                  batch_size = batch_size,\n",
    "                                  shuffle = False,\n",
    "                                  num_workers = num_workers,\n",
    "                                  pin_memory = True\n",
    "                                  )\n",
    "    \n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pizza', 'steak', 'sushi']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from going_modular import data_setup\n",
    "from torchvision import transforms\n",
    "\n",
    "train_dir = \"./data/pizza_steak_sushi/train/\"\n",
    "test_dir = \"./data/pizza_steak_sushi/test/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size = (224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir = train_dir,\n",
    "    test_dir = test_dir,\n",
    "    transform = transform,\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../going_modular/model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../going_modular/model_builder.py\n",
    "\"\"\"Contains pytorch code to develop TinyVGG architecture\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture copying TinyVGG from: \n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_channels: int = 3, \n",
    "                 hidden_units: int = 10,\n",
    "                 num_classes: int = 3):\n",
    "        \"\"\"Class constructor\n",
    "\n",
    "        Args:\n",
    "            num_channels (int): Number of color channels\n",
    "            hidden_units (int): Number of hidden units in model\n",
    "            num_classes (int): Number of labels. Set to 3 for pizza, steak, sushi\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_channels, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3, # how big is the square that's going over the image?\n",
    "                      stride=1, # default\n",
    "                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2) # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from? \n",
    "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "            nn.Linear(in_features=hidden_units*16*16,\n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Overrides forward method from parent. Runs a forward pass\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Image data in tensor format\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits\n",
    "        \"\"\"\n",
    "        # x = self.conv_block_1(x)\n",
    "        # # print(x.shape)\n",
    "        # x = self.conv_block_2(x)\n",
    "        # # print(x.shape)\n",
    "        # x = self.classifier(x)\n",
    "        # # print(x.shape)\n",
    "        return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion\n",
    "\n",
    "class TinyVGG2(nn.Module):\n",
    "    \"\"\"Extension of the TinyVGG model with double the\n",
    "    number of hidden units.\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "  def __init__(self, \n",
    "               num_color_channels: int = 3, \n",
    "               hidden_units: int = 20, \n",
    "               num_classes: int = 3):\n",
    "    super().__init__()\n",
    "    self.conv_block_1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels = num_color_channels,\n",
    "                  out_channels = hidden_units,\n",
    "                  kernel_size = 3,\n",
    "                  stride = 1,\n",
    "                  padding = 1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels = hidden_units,\n",
    "                  out_channels = hidden_units,\n",
    "                  kernel_size = 3,\n",
    "                  stride = 1,\n",
    "                  padding = 1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size = 2,\n",
    "                     stride = 2)\n",
    "    )\n",
    "    self.conv_block_2 = nn.Sequential(\n",
    "        nn.Conv2d(hidden_units, hidden_units, kernel_size = 3, padding = 1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(hidden_units, hidden_units, kernel_size = 3, padding =1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features = hidden_units * 56 * 56,\n",
    "                  out_features = num_classes)\n",
    "    )\n",
    "  def forward(self, x: torch.Tensor):\n",
    "      return self.classifier(self.conv_block_2(self.conv_block_1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TinyVGG2                                 [32, 3]                   --\n",
       "├─Sequential: 1-1                        [32, 20, 112, 112]        --\n",
       "│    └─Conv2d: 2-1                       [32, 20, 224, 224]        560\n",
       "│    └─ReLU: 2-2                         [32, 20, 224, 224]        --\n",
       "│    └─Conv2d: 2-3                       [32, 20, 224, 224]        3,620\n",
       "│    └─ReLU: 2-4                         [32, 20, 224, 224]        --\n",
       "│    └─MaxPool2d: 2-5                    [32, 20, 112, 112]        --\n",
       "├─Sequential: 1-2                        [32, 20, 56, 56]          --\n",
       "│    └─Conv2d: 2-6                       [32, 20, 112, 112]        3,620\n",
       "│    └─ReLU: 2-7                         [32, 20, 112, 112]        --\n",
       "│    └─Conv2d: 2-8                       [32, 20, 112, 112]        3,620\n",
       "│    └─ReLU: 2-9                         [32, 20, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-10                   [32, 20, 56, 56]          --\n",
       "├─Sequential: 1-3                        [32, 3]                   --\n",
       "│    └─Flatten: 2-11                     [32, 62720]               --\n",
       "│    └─Linear: 2-12                      [32, 3]                   188,163\n",
       "==========================================================================================\n",
       "Total params: 199,583\n",
       "Trainable params: 199,583\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 9.62\n",
       "==========================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 642.25\n",
       "Params size (MB): 0.80\n",
       "Estimated Total Size (MB): 662.32\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "from going_modular import model_builder\n",
    "sys.path.append(os.path.join(os.getcwd(),\n",
    "                             \"../src\"))\n",
    "from utils import get_device\n",
    "device = get_device()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.mps.manual_seed(42)\n",
    "model = model_builder.TinyVGG2(num_color_channels=3,\n",
    "                               hidden_units = 20,\n",
    "                               num_classes = len(class_names)).to(device)\n",
    "summary(model, input_size = [32, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 224, 224]), torch.Size([32]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = next(iter(train_dataloader))\n",
    "img.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([32, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(img)\n",
    "type(out), out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrting our training and testing step functions into a python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../going_modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../going_modular/engine.py\n",
    "\"\"\"Contains functions for training and testing a PyTorch model\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: str) -> Tuple[float, float]:\n",
    "    \"\"\"Helper function to train pytorch model on device\n",
    "    and acquire training metrics per epoch\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): instantiated torch model\n",
    "        dataloader (torch.utils.data.DataLoader)\n",
    "        loss_fn (torch.nn.Module)\n",
    "        optimizer (torch.optim.Optimizer) \n",
    "        device (str): Torch device\n",
    "\n",
    "    Returns:\n",
    "        Average training loss and training accuracy per epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    train_loss, train_acc = 0,0\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        # Forward pass\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X) #logits\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute metric across all batches\n",
    "        y_pred_class = torch.argmax(\n",
    "            torch.softmax(y_pred, dim = 1),\n",
    "            dim = 1\n",
    "        )\n",
    "\n",
    "        train_acc += (y_pred_class==y).sum().item()/len(y_pred)\n",
    "    \n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss.cpu() / len(dataloader)\n",
    "    train_acc = train_acc.cpu() /len(dataloader)\n",
    "\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: str) -> Tuple[float, float]:\n",
    "    \"\"\"Runs inference of trained model on test dataset per epoch\n",
    "    and monitors model test metrics.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): instantiated torch model\n",
    "        dataloader (torch.utils.data.DataLoader)\n",
    "        loss_fn (torch.nn.Module)\n",
    "        device (str, optional): _description_. Defaults to device.\n",
    "\n",
    "    Returns:\n",
    "        Average test loss and test accuracy per epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    test_loss, test_acc = 0,0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            \n",
    "            # Forward pass\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            test_pred = model(X) #logits\n",
    "\n",
    "            # Compute metrics \n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            test_pred_class = torch.argmax(\n",
    "                torch.softmax(test_pred, dim = 1),\n",
    "                dim = 1\n",
    "            )\n",
    "            test_acc += (test_pred_class == y).sum().item()/len(test_pred_class)\n",
    "    \n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss/len(dataloader)\n",
    "    test_acc = test_acc/len(dataloader)\n",
    "    \n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          epochs: int,\n",
    "          device: str,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss()): \n",
    "    \"\"\"Wrapper function to train model over specified number of epochs,\n",
    "    model, dataloaders, optimizer and loss function.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): instantiated torch model\n",
    "        train_dataloader (torch.utils.data.DataLoader)\n",
    "        test_dataloader (torch.utils.data.DataLoader)\n",
    "        optimizer (torch.optim.Optimizer)\n",
    "        epochs (int): Number of epochs for training\n",
    "        loss_fn (torch.nn.Module, optional) Defaults to nn.CrossEntropyLoss().\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create storage results dictionary\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    # Loop through training and testing steps for number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model = model,\n",
    "                                           dataloader = train_dataloader,\n",
    "                                           loss_fn = loss_fn,\n",
    "                                           optimizer = optimizer,\n",
    "                                           device = device)\n",
    "        test_loss, test_acc = test_step(model = model,\n",
    "                                        dataloader = test_dataloader,\n",
    "                                        loss_fn = loss_fn,\n",
    "                                        device = device)\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test functions imported!\n"
     ]
    }
   ],
   "source": [
    "from going_modular import engine\n",
    "print(\"Train and test functions imported!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning our utility functions for saving a model into a python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../going_modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../going_modular/utils.py\n",
    "\"\"\"Utility functions for torch driven computer vision projects\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def save_model(root_dir: str,\n",
    "               model_name: str,\n",
    "               model: torch.nn.Module):\n",
    "    \"\"\"Saves a torch model to the given root directory\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Name of directory to store model\n",
    "        model_name (str): Name of the model\n",
    "        model (torch.nn.Module): Torch model object\n",
    "    \"\"\"\n",
    "    MODEL_PATH = Path(root_dir)\n",
    "    MODEL_PATH.mkdir(parents=True,\n",
    "                     exist_ok = True)\n",
    "    MODEL_NAME = model_name\n",
    "    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "    torch.save(obj = model.state_dict(),\n",
    "            f = MODEL_SAVE_PATH)\n",
    "    print(f\"Saved model to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils imported!\n"
     ]
    }
   ],
   "source": [
    "from going_modular import utils\n",
    "print(\"Utils imported!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src.utils import get_device\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../going_modular/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../going_modular/train.py\n",
    "\"\"\"Trains a PyTorch tinyVGG image classification model\n",
    "using device agnostic code\n",
    "\"\"\"\n",
    "import os\n",
    "import get_data, data_setup, engine, model_builder, utils\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src.utils import get_device\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--seed\",\n",
    "                    type = int,\n",
    "                    required = False,\n",
    "                    help = \"Random seed for reproducible experiments\")\n",
    "parser.add_argument(\"--epochs\",\n",
    "                    type = int,\n",
    "                    required = True,\n",
    "                    help = \"Total number of epochs for training\")\n",
    "parser.add_argument(\"--batch_size\",\n",
    "                    type = int,\n",
    "                    required = False,\n",
    "                    default = 32,\n",
    "                    help = \"Batch size for model training\")\n",
    "parser.add_argument(\"--lr\",\n",
    "                    type = float,\n",
    "                    required = True,\n",
    "                    help = \"Learning rate for torch optimizer\")\n",
    "parser.add_argument(\"--workers\",\n",
    "                    type = int,\n",
    "                    required = False,\n",
    "                    default = 0,\n",
    "                    help = \"Number of workers for training\")\n",
    "parser.add_argument(\"--url\",\n",
    "                    type = str,\n",
    "                    required = False,\n",
    "                    default = \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                    help = \"url for data download\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "#Hyperparameters\n",
    "BATCH_SIZE = args.batch_size\n",
    "LEARNING_RATE = args.lr\n",
    "EPOCHS = args.epochs\n",
    "NUM_WORKERS = args.workers\n",
    "URL = args.url\n",
    "if args.seed:\n",
    "    SEED = args.seed\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "HIDDEN_UNITS = 20\n",
    "\n",
    "#Get data\n",
    "data_path = \"data\"\n",
    "image_dir = \"pizza_steak_sushi\"\n",
    "return_ = get_data.main(url,\n",
    "                        data_path,\n",
    "                        image_dir)\n",
    "\n",
    "#Setup directores\n",
    "train_dir = f\"{data_path}/{image_dir}/train\"\n",
    "test_dir = f\"{data_path}/{image_dir}/test\"\n",
    "\n",
    "#Get device\n",
    "device = get_device()\n",
    "if device == \"mps\":\n",
    "    torch.mps.manual_seed(SEED)\n",
    "elif device == 'cuda':\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "#Create transforms\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "#Create dataloaders and get class names\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir = train_dir,\n",
    "    test_dir = test_dir,\n",
    "    transform = data_transform,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_workers = NUM_WORKERS\n",
    ")\n",
    "\n",
    "#Create model\n",
    "model = model_builder.TinyVGG2(\n",
    "    num_color_channels = 3,\n",
    "    hidden_units = HIDDEN_UNITS,\n",
    "    num_classes = 3\n",
    ").to(device)\n",
    "\n",
    "#Setup loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr = LEARNING_RATE)\n",
    "\n",
    "#Train model\n",
    "start_time = timer()\n",
    "engine.train(model = model,\n",
    "             train_dataloader = train_dataloader,\n",
    "             test_dataloader = test_dataloader,\n",
    "             loss_fn = loss_fn,\n",
    "             optimizer = optimizer,\n",
    "             epochs = EPOCHS,\n",
    "             device = device)\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time - start_time:.3f} seconds\")\n",
    "\n",
    "#Save model to file\n",
    "utils.save_model(model = model,\n",
    "                 root_dir = \"models\",\n",
    "                 model_name = \"tinyVGG2_pizza_steak_sushi.pth\")\n",
    "\n",
    "print(\"Model saved in models directory as tinyVGG2_pizza_steak_sushi.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73eba0fcdc9843ca885e5e215ae31591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Mismatched Tensor types in NNPack convolutionOutput",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Personal Learning Folder/Data Science/15. PyTorch Developer Class/symmetrical-octo-spork/going_modular/train.py:95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m#Train model\u001b[39;00m\n\u001b[1;32m     94\u001b[0m start_time \u001b[39m=\u001b[39m timer()\n\u001b[0;32m---> 95\u001b[0m engine\u001b[39m.\u001b[39;49mtrain(model \u001b[39m=\u001b[39;49m model,\n\u001b[1;32m     96\u001b[0m              train_dataloader \u001b[39m=\u001b[39;49m train_dataloader,\n\u001b[1;32m     97\u001b[0m              test_dataloader \u001b[39m=\u001b[39;49m test_dataloader,\n\u001b[1;32m     98\u001b[0m              loss_fn \u001b[39m=\u001b[39;49m loss_fn,\n\u001b[1;32m     99\u001b[0m              optimizer \u001b[39m=\u001b[39;49m optimizer,\n\u001b[1;32m    100\u001b[0m              epochs \u001b[39m=\u001b[39;49m EPOCHS,\n\u001b[1;32m    101\u001b[0m              device \u001b[39m=\u001b[39;49m device)\n\u001b[1;32m    102\u001b[0m end_time \u001b[39m=\u001b[39m timer()\n\u001b[1;32m    103\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTotal training time: \u001b[39m\u001b[39m{\u001b[39;00mend_time\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart_time\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Personal Learning Folder/Data Science/15. PyTorch Developer Class/symmetrical-octo-spork/going_modular/engine.py:128\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, epochs, device, loss_fn)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39m# Loop through training and testing steps for number of epochs\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[0;32m--> 128\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train_step(model \u001b[39m=\u001b[39;49m model,\n\u001b[1;32m    129\u001b[0m                                        dataloader \u001b[39m=\u001b[39;49m train_dataloader,\n\u001b[1;32m    130\u001b[0m                                        loss_fn \u001b[39m=\u001b[39;49m loss_fn,\n\u001b[1;32m    131\u001b[0m                                        optimizer \u001b[39m=\u001b[39;49m optimizer,\n\u001b[1;32m    132\u001b[0m                                        device \u001b[39m=\u001b[39;49m device)\n\u001b[1;32m    133\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test_step(model \u001b[39m=\u001b[39m model,\n\u001b[1;32m    134\u001b[0m                                     dataloader \u001b[39m=\u001b[39m test_dataloader,\n\u001b[1;32m    135\u001b[0m                                     loss_fn \u001b[39m=\u001b[39m loss_fn,\n\u001b[1;32m    136\u001b[0m                                     device \u001b[39m=\u001b[39m device)\n\u001b[1;32m    137\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    138\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest_acc: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Personal Learning Folder/Data Science/15. PyTorch Developer Class/symmetrical-octo-spork/going_modular/engine.py:36\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     33\u001b[0m     \n\u001b[1;32m     34\u001b[0m     \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[39m# X, y = X.to(device), y.to(device)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     y_pred \u001b[39m=\u001b[39m model(X) \u001b[39m#logits\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(y_pred, y)\n\u001b[1;32m     38\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Personal Learning Folder/Data Science/15. PyTorch Developer Class/symmetrical-octo-spork/going_modular/model_builder.py:105\u001b[0m, in \u001b[0;36mTinyVGG2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_block_2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_block_1(x)))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Mismatched Tensor types in NNPack convolutionOutput"
     ]
    }
   ],
   "source": [
    "%run ../going_modular/train.py --seed 42 --epochs 20 --lr 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
